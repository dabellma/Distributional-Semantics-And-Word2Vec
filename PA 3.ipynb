{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985ef66f",
   "metadata": {},
   "source": [
    "# 1 Create distributional semantic word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8bda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330fcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = [\"the men feed the dogs\",\n",
    "\"the women feed the dogs\",\n",
    "\"the women feed the men\",\n",
    "\"the men feed the men\",\n",
    "\"the dogs bite the men\",\n",
    "\"the dogs bite the women\",\n",
    "\"the dogs bite the dogs\",\n",
    "\"the dogs like the men\",\n",
    "\"the men like the women\",\n",
    "\"the women like the dogs\",\n",
    "\"the men like the dogs\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eaa080b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feed': 0, 'the': 1, 'women': 2, 'bite': 3, 'men': 4, 'like': 5, 'dogs': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = list(set([item for sublist in [s.split() for s in sim_data] for item in sublist]))\n",
    "voc_dict = {voc[i]:i for i in range(len(voc))}\n",
    "inv_voc_dict = {i:voc[i] for i in range(len(voc))}\n",
    "voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819a9b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feed', 'the', 'women', 'bite', 'men', 'like', 'dogs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc8942c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = np.zeros((len(voc),len(voc)))\n",
    "cm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09071ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all indexed occurrences of a word in a sentence\n",
    "def get_indexed_occurrences(word, sentence):\n",
    "    indexed_sentence = sentence.split()\n",
    "    indices = [i for i, w in enumerate(indexed_sentence) if w == word]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55bde189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# between two lists of indices, determine if there's a difference of 1 among all combinations of the two lists elements\n",
    "# for example, \"the men feed the dogs\" has 2 \"the\"s. I want to ensure both \"the\" words are accounted for being by \n",
    "# women and dogs, respectively\n",
    "def distance_of_one_exists(list1, list2):\n",
    "    for value1 in list1:\n",
    "        for value2 in list2:\n",
    "            if (abs(value1 - value2) == 1):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b54805c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 4., 2., 0., 2., 0., 0.],\n",
       "       [4., 0., 5., 3., 7., 4., 8.],\n",
       "       [2., 5., 0., 0., 0., 1., 0.],\n",
       "       [0., 3., 0., 0., 0., 0., 3.],\n",
       "       [2., 7., 0., 0., 0., 2., 0.],\n",
       "       [0., 4., 1., 0., 2., 0., 1.],\n",
       "       [0., 8., 0., 3., 0., 1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for w in voc:\n",
    "    for c in voc:\n",
    "        #if it's not the same word\n",
    "        if w != c:\n",
    "            #for each sentence in corpus\n",
    "            for sent in sim_data:\n",
    "                #if both words, w and c, are in the sentence\n",
    "                if w in sent.split() and c in sent.split(): \n",
    "                    #if they are next to each other\n",
    "                    if distance_of_one_exists(get_indexed_occurrences(w, sent), get_indexed_occurrences(c, sent)):\n",
    "                            cm[voc_dict[w],voc_dict[c]] += 1\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34aae955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 41., 21.,  1., 21.,  1.,  1.],\n",
       "       [41.,  1., 51., 31., 71., 41., 81.],\n",
       "       [21., 51.,  1.,  1.,  1., 11.,  1.],\n",
       "       [ 1., 31.,  1.,  1.,  1.,  1., 31.],\n",
       "       [21., 71.,  1.,  1.,  1., 21.,  1.],\n",
       "       [ 1., 41., 11.,  1., 21.,  1., 11.],\n",
       "       [ 1., 81.,  1., 31.,  1., 11.,  1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm *= 10\n",
    "cm += 1\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57c1c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.360679774997898\n",
      "46.9041575982343\n",
      "38.72983346207417\n",
      "14.142135623730951\n",
      "42.42640687119285\n",
      "31.622776601683793\n"
     ]
    }
   ],
   "source": [
    "women_and_men_cm = scipy.linalg.norm(cm[2] - cm[4])\n",
    "women_and_dogs_cm = scipy.linalg.norm(cm[2] - cm[6])\n",
    "men_and_dogs_cm = scipy.linalg.norm(cm[4] - cm[6])\n",
    "feed_and_like_cm = scipy.linalg.norm(cm[0] - cm[5])\n",
    "feed_and_bite_cm = scipy.linalg.norm(cm[0] - cm[3])\n",
    "like_and_bite_cm = scipy.linalg.norm(cm[5] - cm[3])\n",
    "\n",
    "print(women_and_men_cm)\n",
    "print(women_and_dogs_cm)\n",
    "print(men_and_dogs_cm)\n",
    "print(feed_and_like_cm)\n",
    "print(feed_and_bite_cm)\n",
    "print(like_and_bite_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89a491aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_PPMI(w, c, cooccurrence_matrix):\n",
    "    N = np.sum(cooccurrence_matrix)\n",
    "    \n",
    "    numerator = cooccurrence_matrix[w,c] / N\n",
    "    denominator = (np.sum(cooccurrence_matrix[w,:]) / N ) * (np.sum(cooccurrence_matrix[:,c]) / N)\n",
    "    \n",
    "    return max(np.log2(numerator / denominator), 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4465e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPMI_matrix = np.zeros((cm.shape[0], cm.shape[1]))\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        PPMI_matrix[i, j] = calc_PPMI(i, j, cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c93233",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9abdc778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.40230909, 1.30247004, 0.        , 0.87504882, 0.        , 0.        ],\n",
       "       [0.40230909, 0.        , 0.71718242, 0.3758077 , 0.76708298, 0.40230909, 0.83886589],\n",
       "       [1.30247004, 0.71718242, 0.        , 0.        , 0.        , 0.36958424, 0.        ],\n",
       "       [0.        , 0.3758077 , 0.        , 0.        , 0.        , 0.        , 1.69546204],\n",
       "       [0.87504882, 0.76708298, 0.        , 0.        , 0.        , 0.87504882, 0.        ],\n",
       "       [0.        , 0.40230909, 0.36958424, 0.        , 0.87504882, 0.        , 0.        ],\n",
       "       [0.        , 0.83886589, 0.        , 1.69546204, 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPMI_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2ab0dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feed': 0, 'the': 1, 'women': 2, 'bite': 3, 'men': 4, 'like': 5, 'dogs': 6}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ca5c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6638323657475999\n",
      "2.1731127708616107\n",
      "2.100277406500883\n",
      "0.9328858041414629\n",
      "2.310288400824855\n",
      "1.9436040742520886\n"
     ]
    }
   ],
   "source": [
    "women_and_men = scipy.linalg.norm(PPMI_matrix[2] - PPMI_matrix[4])\n",
    "women_and_dogs = scipy.linalg.norm(PPMI_matrix[2] - PPMI_matrix[6])\n",
    "men_and_dogs = scipy.linalg.norm(PPMI_matrix[4] - PPMI_matrix[6])\n",
    "feed_and_like= scipy.linalg.norm(PPMI_matrix[0] - PPMI_matrix[5])\n",
    "feed_and_bite= scipy.linalg.norm(PPMI_matrix[0] - PPMI_matrix[3])\n",
    "like_and_bite= scipy.linalg.norm(PPMI_matrix[5] - PPMI_matrix[3])\n",
    "\n",
    "print(women_and_men)\n",
    "print(women_and_dogs)\n",
    "print(men_and_dogs)\n",
    "print(feed_and_like)\n",
    "print(feed_and_bite)\n",
    "print(like_and_bite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de4baba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, E, Vt = scipy.linalg.svd(PPMI_matrix, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb130443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.39948604, -0.44884398,  0.34290825,  0.36263918,  0.27086762,  0.39715228, -0.39844391],\n",
       "        [-0.49350358, -0.32522982, -0.07643136, -0.02679549, -0.78340307, -0.08847937,  0.15023668],\n",
       "        [-0.37985835,  0.46889795, -0.26936487,  0.30519918,  0.09215832,  0.50020364,  0.45982773],\n",
       "        [-0.32171275, -0.28333444, -0.5810479 , -0.57326033,  0.37703255,  0.05347619, -0.07046403],\n",
       "        [-0.37426426,  0.43553137, -0.23999154,  0.27240819,  0.01918632, -0.48815204, -0.54751804],\n",
       "        [-0.26080618, -0.22080862,  0.19725361,  0.19746777,  0.40144034, -0.58471223,  0.54979854],\n",
       "        [-0.37547312,  0.39265371,  0.60818842, -0.57688919,  0.02722492,  0.03460171, -0.0144996 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = np.matrix(U)\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c838d694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2.55526972, 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.91824751, 0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 1.72522246, 0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 1.72376078, 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.65843145, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        , 0.47525179, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.45238088]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = np.matrix(np.diag(E))\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c262cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.39948604, -0.49350358, -0.37985835, -0.32171275, -0.37426426, -0.26080618, -0.37547312],\n",
       "        [ 0.44884398,  0.32522982, -0.46889795,  0.28333444, -0.43553137,  0.22080862, -0.39265371],\n",
       "        [-0.34290825,  0.07643136,  0.26936487,  0.5810479 ,  0.23999154, -0.19725361, -0.60818842],\n",
       "        [ 0.36263918, -0.02679549,  0.30519918, -0.57326033,  0.27240819,  0.19746777, -0.57688919],\n",
       "        [-0.27086762,  0.78340307, -0.09215832, -0.37703255, -0.01918632, -0.40144034, -0.02722492],\n",
       "        [ 0.39715228, -0.08847937,  0.50020364,  0.05347619, -0.48815204, -0.58471223,  0.03460171],\n",
       "        [ 0.39844391, -0.15023668, -0.45982773,  0.07046403,  0.54751804, -0.54979854,  0.0144996 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vt = np.matrix(Vt)\n",
    "Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03ddb13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = Vt.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e935be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7, 7), (7, 7), (7, 7))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, E.shape, Vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "281c0883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.38777878e-17,  4.02309087e-01,  1.30247004e+00,  7.16440796e-16,  8.75048816e-01,  2.63677968e-16, -7.70217223e-16],\n",
       "        [ 4.02309087e-01, -2.42861287e-16,  7.17182425e-01,  3.75807699e-01,  7.67082979e-01,  4.02309087e-01,  8.38865895e-01],\n",
       "        [ 1.30247004e+00,  7.17182425e-01,  3.46944695e-16, -3.52148866e-16,  2.22044605e-16,  3.69584236e-01,  3.66460334e-16],\n",
       "        [ 2.06432094e-16,  3.75807699e-01, -3.36536354e-16,  4.34548231e-16, -5.37764278e-16,  2.63677968e-16,  1.69546204e+00],\n",
       "        [ 8.75048816e-01,  7.67082979e-01,  6.52256027e-16,  3.71230824e-16,  8.60422844e-16,  8.75048816e-01,  7.19910243e-17],\n",
       "        [-6.93889390e-17,  4.02309087e-01,  3.69584236e-01,  2.74086309e-16,  8.75048816e-01, -1.11022302e-16, -1.18394877e-16],\n",
       "        [-3.52582546e-16,  8.38865895e-01,  3.47378376e-16,  1.69546204e+00,  5.25621213e-16, -2.15105711e-16,  9.59654448e-16]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(U @ E) @ Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdb6fe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-1.02079459, -0.86099384,  0.59159301],\n",
       "        [-1.26103475, -0.62387128, -0.1318611 ],\n",
       "        [-0.97064055,  0.89946233, -0.46471431],\n",
       "        [-0.82206286, -0.54350558, -1.00243689],\n",
       "        [-0.95634612,  0.83545697, -0.4140388 ],\n",
       "        [-0.66643013, -0.42356559,  0.34030635],\n",
       "        [-0.95943508,  0.75320699,  1.04926033]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_PPMI = PPMI_matrix @ V[:,0:3]\n",
    "reduced_PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21b1b96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08287957673775763\n",
      "1.5210639044886252\n",
      "1.4656121329992486\n",
      "0.6164921946291915\n",
      "1.6374444803749935\n",
      "1.357043319290905\n"
     ]
    }
   ],
   "source": [
    "women_and_men_reduced = scipy.linalg.norm(reduced_PPMI[2] - reduced_PPMI[4])\n",
    "women_and_dogs_reduced = scipy.linalg.norm(reduced_PPMI[2] - reduced_PPMI[6])\n",
    "men_and_dogs_reduced = scipy.linalg.norm(reduced_PPMI[4] - reduced_PPMI[6])\n",
    "feed_and_like_reduced = scipy.linalg.norm(reduced_PPMI[0] - reduced_PPMI[5])\n",
    "feed_and_bite_reduced = scipy.linalg.norm(reduced_PPMI[0] - reduced_PPMI[3])\n",
    "like_and_bite_reduced = scipy.linalg.norm(reduced_PPMI[5] - reduced_PPMI[3])\n",
    "\n",
    "print(women_and_men_reduced)\n",
    "print(women_and_dogs_reduced)\n",
    "print(men_and_dogs_reduced)\n",
    "print(feed_and_like_reduced)\n",
    "print(feed_and_bite_reduced)\n",
    "print(like_and_bite_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a93723",
   "metadata": {},
   "source": [
    "# 2 Computing with distributional semantic word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc45554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"GoogleNews-vectors-rcv_vocab.txt\", 'r') as file:\n",
    "    google_dictionary = {}\n",
    "    \n",
    "    #get rid of first line causing errors\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        key = words[0]\n",
    "        values = [word for word in words[1:]]\n",
    "\n",
    "        google_dictionary[key] = np.array(values, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de5b2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"EN-wform.w.2.ppmi.svd.500.rcv_vocab.txt\", 'r') as file:\n",
    "    composes_dictionary = {}\n",
    "\n",
    "    #get rid of first line causing errors\n",
    "    next(file)\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        key = words[0]\n",
    "        values = [word for word in words[1:]]\n",
    "\n",
    "        composes_dictionary[key] = np.array(values, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86b8fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dict = {'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6}\n",
    "\n",
    "file_path = 'SAT-package-V3.txt' \n",
    "current_paragraph = []\n",
    "paragraphs = {}\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    i = 0\n",
    "    for line in file:\n",
    "        if line.startswith('#') or not line.strip():\n",
    "            if len(current_paragraph) != 0:\n",
    "                paragraphs[i] = current_paragraph\n",
    "                i+=1\n",
    "            current_paragraph = []\n",
    "            continue\n",
    "        else:\n",
    "            current_paragraph.append(line.strip())\n",
    "\n",
    "for key, paragraph in paragraphs.items():\n",
    "    paragraph[7] = answer_dict[paragraph[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "87dbf4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['190 FROM REAL SATs', 'lull trust v:n', 'balk fortitude v:n', 'betray loyalty v:n', 'cajole compliance v:n', 'hinder destination v:n', 'soothe passion v:n', 4]\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0cfe8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a,b):\n",
    "    return np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a8881a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take cos similarity between word and proposed analogous word and then find diffs\n",
    "def diff_of_cos_choices_compared_to_cos_of_question(paragraph, vectors):\n",
    "        diff_question = cos_sim(vectors[paragraph[1].split()[0]],vectors[paragraph[1].split()[1]])\n",
    "\n",
    "        diff_a = cos_sim(vectors[paragraph[2].split()[0]],vectors[paragraph[2].split()[1]])\n",
    "        diff_b = cos_sim(vectors[paragraph[3].split()[0]],vectors[paragraph[3].split()[1]])\n",
    "        diff_c = cos_sim(vectors[paragraph[4].split()[0]],vectors[paragraph[4].split()[1]])\n",
    "        diff_d = cos_sim(vectors[paragraph[5].split()[0]],vectors[paragraph[5].split()[1]])\n",
    "        diff_e = cos_sim(vectors[paragraph[6].split()[0]],vectors[paragraph[6].split()[1]])\n",
    "\n",
    "        abs_a = abs(diff_question - diff_a)\n",
    "        abs_b = abs(diff_question - diff_b)\n",
    "        abs_c = abs(diff_question - diff_c)\n",
    "        abs_d = abs(diff_question - diff_d)\n",
    "        abs_e = abs(diff_question - diff_e)\n",
    "\n",
    "        return abs_a, abs_b, abs_c, abs_d, abs_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5bc3ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take euc distance similarity between word and proposed analogous word and then find diffs\n",
    "def diff_of_euc_distance_of_choices_compared_to_euc_distance_of_question(paragraph, vectors):\n",
    "        diff_question_euc = scipy.linalg.norm(vectors[paragraph[1].split()[0]] - vectors[paragraph[1].split()[1]])\n",
    "        \n",
    "        diff_a_euc = scipy.linalg.norm(vectors[paragraph[2].split()[0]] - vectors[paragraph[2].split()[1]])\n",
    "        diff_b_euc = scipy.linalg.norm(vectors[paragraph[3].split()[0]] - vectors[paragraph[3].split()[1]])\n",
    "        diff_c_euc = scipy.linalg.norm(vectors[paragraph[4].split()[0]] - vectors[paragraph[4].split()[1]])\n",
    "        diff_d_euc = scipy.linalg.norm(vectors[paragraph[5].split()[0]] - vectors[paragraph[5].split()[1]])\n",
    "        diff_e_euc = scipy.linalg.norm(vectors[paragraph[6].split()[0]] - vectors[paragraph[6].split()[1]])\n",
    "\n",
    "        abs_a_euc = abs(diff_question_euc - diff_a_euc)\n",
    "        abs_b_euc = abs(diff_question_euc - diff_b_euc)\n",
    "        abs_c_euc = abs(diff_question_euc - diff_c_euc)\n",
    "        abs_d_euc = abs(diff_question_euc - diff_d_euc)\n",
    "        abs_e_euc = abs(diff_question_euc - diff_e_euc)\n",
    "\n",
    "        return abs_a_euc, abs_b_euc, abs_c_euc, abs_d_euc, abs_e_euc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1547d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diffs of model vectors, and then cos similarity among the diffs of model vectors\n",
    "def diff_of_subtract_then_cos_similarity(paragraph, vectors):\n",
    "    diff_question = vectors[paragraph[1].split()[0]] - vectors[paragraph[1].split()[1]]\n",
    "    \n",
    "    diff_a = vectors[paragraph[2].split()[0]] - vectors[paragraph[2].split()[1]]\n",
    "    diff_b = vectors[paragraph[3].split()[0]] - vectors[paragraph[3].split()[1]]\n",
    "    diff_c = vectors[paragraph[4].split()[0]] - vectors[paragraph[4].split()[1]]\n",
    "    diff_d = vectors[paragraph[5].split()[0]] - vectors[paragraph[5].split()[1]]\n",
    "    diff_e = vectors[paragraph[6].split()[0]] - vectors[paragraph[6].split()[1]]\n",
    "    \n",
    "    simularity_diffs_a = cos_sim(diff_question, diff_a)\n",
    "    simularity_diffs_b = cos_sim(diff_question, diff_b)    \n",
    "    simularity_diffs_c = cos_sim(diff_question, diff_c)    \n",
    "    simularity_diffs_d = cos_sim(diff_question, diff_d)    \n",
    "    simularity_diffs_e = cos_sim(diff_question, diff_e)    \n",
    "    \n",
    "    return simularity_diffs_a, simularity_diffs_b, simularity_diffs_c, simularity_diffs_d, simularity_diffs_e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b609ae6a",
   "metadata": {},
   "source": [
    "Google, best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "980548a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 144, 0.4418604651162791)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right = 0\n",
    "wrong = 0\n",
    "num_sat_questions = len(paragraphs)\n",
    "\n",
    "for i in range(num_sat_questions):\n",
    "    if paragraphs[i][1].split()[0] in google_dictionary and \\\n",
    "        paragraphs[i][1].split()[1] in google_dictionary and \\\n",
    "        paragraphs[i][2].split()[0] in google_dictionary and \\\n",
    "        paragraphs[i][2].split()[1] in google_dictionary and \\\n",
    "        paragraphs[i][3].split()[0] in google_dictionary and \\\n",
    "        paragraphs[i][3].split()[1] in google_dictionary and \\\n",
    "        paragraphs[i][4].split()[0] in google_dictionary and \\\n",
    "        paragraphs[i][4].split()[1] in google_dictionary and \\\n",
    "        paragraphs[i][5].split()[0] in google_dictionary and \\\n",
    "        paragraphs[i][5].split()[1] in google_dictionary and \\\n",
    "        paragraphs[i][6].split()[0] in google_dictionary and \\\n",
    "        paragraphs[i][6].split()[1] in google_dictionary:\n",
    "        \n",
    "        \n",
    "        abs_a, abs_b, abs_c, abs_d, abs_e = diff_of_cos_choices_compared_to_cos_of_question(paragraphs[i], google_dictionary)\n",
    "        abs_a_euc, abs_b_euc, abs_c_euc, abs_d_euc, abs_e_euc = diff_of_euc_distance_of_choices_compared_to_euc_distance_of_question(paragraphs[i], google_dictionary)\n",
    "        simularity_diffs_a, simularity_diffs_b, simularity_diffs_c, simularity_diffs_d, simularity_diffs_e = diff_of_subtract_then_cos_similarity(paragraphs[i], google_dictionary)\n",
    "        \n",
    "        cos_diff_array_of_choices = [abs_a, abs_b, abs_c, abs_d, abs_e]\n",
    "        euc_diff_array_of_choices = [abs_a_euc, abs_b_euc, abs_c_euc, abs_d_euc, abs_e_euc]\n",
    "        simularity_diffs_cos_array_of_choices = [simularity_diffs_a, simularity_diffs_b, simularity_diffs_c, simularity_diffs_d, simularity_diffs_e]\n",
    "        \n",
    "#         predicted_answer = cos_diff_array_of_choices.index(min(abs_a, abs_b, abs_c, abs_d, abs_e)) + 2\n",
    "#         predicted_answer = euc_diff_array_of_choices.index(min(abs_a_euc, abs_b_euc, abs_c_euc, abs_d_euc, abs_e_euc)) + 2\n",
    "        predicted_answer = simularity_diffs_cos_array_of_choices.index(max(simularity_diffs_a, simularity_diffs_b, simularity_diffs_c, simularity_diffs_d, simularity_diffs_e)) + 2\n",
    "        \n",
    "    \n",
    "    \n",
    "        correct_answer = paragraphs[i][7]\n",
    "        if predicted_answer == correct_answer:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "right, wrong, right / (right + wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd0cfa",
   "metadata": {},
   "source": [
    "Composes, best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0bbcb0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 147, 0.43243243243243246)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right = 0\n",
    "wrong = 0\n",
    "num_sat_questions = len(paragraphs)\n",
    "\n",
    "for i in range(num_sat_questions):\n",
    "    if paragraphs[i][1].split()[0] in composes_dictionary and \\\n",
    "        paragraphs[i][1].split()[1] in composes_dictionary and \\\n",
    "        paragraphs[i][2].split()[0] in composes_dictionary and \\\n",
    "        paragraphs[i][2].split()[1] in composes_dictionary and \\\n",
    "        paragraphs[i][3].split()[0] in composes_dictionary and \\\n",
    "        paragraphs[i][3].split()[1] in composes_dictionary and \\\n",
    "        paragraphs[i][4].split()[0] in composes_dictionary and \\\n",
    "        paragraphs[i][4].split()[1] in composes_dictionary and \\\n",
    "        paragraphs[i][5].split()[0] in composes_dictionary and \\\n",
    "        paragraphs[i][5].split()[1] in composes_dictionary and \\\n",
    "        paragraphs[i][6].split()[0] in composes_dictionary and \\\n",
    "        paragraphs[i][6].split()[1] in composes_dictionary:\n",
    "        \n",
    "        \n",
    "        abs_a, abs_b, abs_c, abs_d, abs_e = diff_of_cos_choices_compared_to_cos_of_question(paragraphs[i], composes_dictionary)\n",
    "        abs_a_euc, abs_b_euc, abs_c_euc, abs_d_euc, abs_e_euc = diff_of_euc_distance_of_choices_compared_to_euc_distance_of_question(paragraphs[i], composes_dictionary)\n",
    "        simularity_diffs_a, simularity_diffs_b, simularity_diffs_c, simularity_diffs_d, simularity_diffs_e = diff_of_subtract_then_cos_similarity(paragraphs[i], composes_dictionary)\n",
    "        \n",
    "        cos_diff_array_of_choices = [abs_a, abs_b, abs_c, abs_d, abs_e]\n",
    "        euc_diff_array_of_choices = [abs_a_euc, abs_b_euc, abs_c_euc, abs_d_euc, abs_e_euc]\n",
    "        simularity_diffs_cos_array_of_choices = [simularity_diffs_a, simularity_diffs_b, simularity_diffs_c, simularity_diffs_d, simularity_diffs_e]\n",
    "        \n",
    "#         predicted_answer = cos_diff_array_of_choices.index(min(abs_a, abs_b, abs_c, abs_d, abs_e)) + 2\n",
    "#         predicted_answer = euc_diff_array_of_choices.index(min(abs_a_euc, abs_b_euc, abs_c_euc, abs_d_euc, abs_e_euc)) + 2\n",
    "        predicted_answer = simularity_diffs_cos_array_of_choices.index(max(simularity_diffs_a, simularity_diffs_b, simularity_diffs_c, simularity_diffs_d, simularity_diffs_e)) + 2\n",
    "        \n",
    "    \n",
    "    \n",
    "        correct_answer = paragraphs[i][7]\n",
    "        if predicted_answer == correct_answer:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "right, wrong, right / (right + wrong)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
